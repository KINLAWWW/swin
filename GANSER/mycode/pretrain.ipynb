{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-08 15:35:27] INFO (torcheeg/MainThread) 🔍 | Detected cached processing results, reading cache from .torcheeg/datasets_1737122480267_T387E.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 9, 9])\n",
      "0\n",
      "320\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch import autograd\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "\n",
    "from strokes import StrokePatientsMIDataset\n",
    "from strokesdict import STROKEPATIENTSMI_LOCATION_DICT\n",
    "import scipy\n",
    "from torcheeg.transforms import Select,BandSignal,Compose,ToTensor\n",
    "from to import ToGrid\n",
    "from typing import Callable, Dict, Union, List\n",
    "import numpy as np\n",
    "import soxr\n",
    "from downsample import SetSamplingRate\n",
    "from baseline import BaselineCorrection\n",
    "from torcheeg.transforms import EEGTransform, Select,BandSignal,Compose,ToTensor\n",
    "\n",
    "dataset = StrokePatientsMIDataset(root_path='../../mi_swin/subdataset',\n",
    "                                  io_path='.torcheeg/datasets_1737122480267_T387E',\n",
    "                        chunk_size=500,  # 1 second\n",
    "                        overlap = 0,\n",
    "                        offline_transform=Compose(\n",
    "                                [BaselineCorrection(),\n",
    "                                SetSamplingRate(origin_sampling_rate=500,target_sampling_rate=128),\n",
    "                                BandSignal(sampling_rate=128,band_dict={'frequency_range':[8,40]})\n",
    "                                ]),\n",
    "                        online_transform=Compose(\n",
    "                                # [ToTensor()]),\n",
    "                                [ToGrid(STROKEPATIENTSMI_LOCATION_DICT),ToTensor()]),\n",
    "                \n",
    "                        label_transform=Select('label'),\n",
    "                        num_worker=8\n",
    ")\n",
    "print(dataset[0][0].shape) #EEG shape(1,30,128)\n",
    "print(dataset[0][1])  # label (int)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def delete_folder_if_exists(target_folder_name):\n",
    "    # 获取父文件夹中的所有内容\n",
    "    parent_folder = os.getcwd()\n",
    "    for folder_name in os.listdir(parent_folder):\n",
    "        folder_path = os.path.join(parent_folder, folder_name)\n",
    "\n",
    "        # 检查是否是文件夹并且名称是否匹配\n",
    "        if os.path.isdir(folder_path) and folder_name == target_folder_name:\n",
    "            try:\n",
    "                # 删除目标文件夹\n",
    "                shutil.rmtree(folder_path)\n",
    "                print(f\"已删除文件夹: {folder_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"删除文件夹 {folder_path} 时出错: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, test_size=0.2, random_state=520, shuffle=True):\n",
    "    n_samples = len(dataset)\n",
    "    indices = np.arange(n_samples)\n",
    "    train_index, test_index = model_selection.train_test_split(\n",
    "        indices,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        shuffle=shuffle)\n",
    "\n",
    "    trian_dataset = Subset(dataset, train_index)\n",
    "    test_dataset = Subset(dataset, test_index)\n",
    "\n",
    "    return trian_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 64\n"
     ]
    }
   ],
   "source": [
    "sub_dataset, test_dataset = train_test_split(dataset=dataset)\n",
    "print(len(sub_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECEIVED_PARAMS = {\n",
    "    \"c_lr\": 0.00001,\n",
    "    \"g_lr\": 0.00001,\n",
    "    \"d_lr\": 0.00001,\n",
    "    \"weight_gp\": 1.0,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"weight_ssl\": 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,\n",
    "                      128,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=True), nn.LeakyReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            nn.LeakyReLU())\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            nn.LeakyReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.LeakyReLU())\n",
    "        self.delayer1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16 + 32,\n",
    "                               32,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=True), nn.LeakyReLU())\n",
    "        self.delayer2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32 + 64,\n",
    "                               64,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=True), nn.LeakyReLU())\n",
    "        self.delayer3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64 + 128,\n",
    "                               128,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #         x = channel_to_location(x)\n",
    "        mask = (x.abs().sum(dim=1, keepdim=True) > 0).float()\n",
    "        out1 = self.layer1(x)\n",
    "        out2 = self.layer2(out1)\n",
    "        out3 = self.layer3(out2)\n",
    "        out = self.layer4(out3)\n",
    "        out = self.delayer1(torch.cat([out, out3], dim=1))\n",
    "        out = self.delayer2(torch.cat([out, out2], dim=1))\n",
    "        out = self.delayer3(torch.cat([out, out1], dim=1))\n",
    "\n",
    "        return out * mask\n",
    "\n",
    "\n",
    "class ResidualConv2d(nn.Module):  # 貌似并未使用该函数\n",
    "    def __init__(self, in_channels, out_channels, bias=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,\n",
    "                      out_channels,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=bias), nn.SELU(),\n",
    "            nn.Conv2d(out_channels,\n",
    "                      out_channels,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=bias))\n",
    "        self.res = nn.Conv2d(in_channels,\n",
    "                             out_channels,\n",
    "                             kernel_size=1,\n",
    "                             stride=1,\n",
    "                             padding=0,\n",
    "                             bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x) + self.res(x)\n",
    "\n",
    "# 识别情感需要分析不同空间尺度下的EEG信号，故引入了包含三种不同尺寸滤波器的InceptionConv2d来提取多尺度特征图\n",
    "class InceptionConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True):\n",
    "        super().__init__()\n",
    "        self.conv5x5 = nn.Conv2d(in_channels,\n",
    "                                 out_channels,\n",
    "                                 kernel_size=5,\n",
    "                                 stride=1,\n",
    "                                 padding=2,\n",
    "                                 bias=bias)\n",
    "        self.conv3x3 = nn.Conv2d(in_channels,\n",
    "                                 out_channels,\n",
    "                                 kernel_size=3,\n",
    "                                 stride=1,\n",
    "                                 padding=1,\n",
    "                                 bias=bias)\n",
    "        self.conv1x1 = nn.Conv2d(in_channels,\n",
    "                                 out_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0,\n",
    "                                 bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv5x5(x) + self.conv3x3(x) + self.conv1x1(x)\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 padding=1,\n",
    "                 bias=True):\n",
    "        super().__init__()\n",
    "        # 不同时间点的二维数据分别进行卷积\n",
    "        self.depth = nn.Conv2d(in_channels,\n",
    "                               in_channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=stride,\n",
    "                               padding=padding,\n",
    "                               groups=in_channels,\n",
    "                               bias=bias)\n",
    "        # 单个eeg通道跨时间点进行卷积\n",
    "        self.point = nn.Conv2d(in_channels,\n",
    "                               out_channels,\n",
    "                               kernel_size=1,\n",
    "                               stride=stride,\n",
    "                               padding=0,\n",
    "                               bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depth(x)\n",
    "        x = self.point(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=4):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layer1 = nn.Conv2d(in_channels,\n",
    "                                256,\n",
    "                                kernel_size=3,\n",
    "                                stride=1,\n",
    "                                padding=1,\n",
    "                                bias=True)\n",
    "        self.layer2 = nn.Conv2d(256,\n",
    "                                128,\n",
    "                                kernel_size=5,\n",
    "                                stride=1,\n",
    "                                padding=2,\n",
    "                                bias=True)\n",
    "        self.layer3 = nn.Conv2d(128,\n",
    "                                64,\n",
    "                                kernel_size=5,\n",
    "                                stride=1,\n",
    "                                padding=2,\n",
    "                                bias=True)\n",
    "        self.layer4 = SeparableConv2d(64,\n",
    "                                      32,\n",
    "                                      kernel_size=5,\n",
    "                                      stride=1,\n",
    "                                      padding=2,\n",
    "                                      bias=True)\n",
    "        self.layer5 = InceptionConv2d(32, 16)\n",
    "\n",
    "        self.drop = nn.Sequential(nn.SELU())\n",
    "        self.fc1 = nn.Sequential(nn.Linear(7 * 5 * 16, 1024, bias=True),\n",
    "                                 nn.SELU()) # stroke MI dataset 的网格为 7*5\n",
    "        self.fc2 = nn.Linear(1024, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.drop(out)\n",
    "        out = out.view(out.size(0), -1) # (batch_size, num_features)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 128, 9, 9])\n",
      "Output shape: torch.Size([2, 128, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "g_model = Generator(in_channels=128, out_channels=128)\n",
    "# 模拟输入\n",
    "input_tensor = torch.randn(2, 128, 9, 9)  # [batch, 1, 128, 7, 5]\n",
    "output = g_model(input_tensor)\n",
    "\n",
    "print(\"Input shape:\", input_tensor.shape)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mask(data, min_r=0.0, max_r=0.5):\n",
    "    # batch_size*channel_num*time_step\n",
    "    data = data.clone()\n",
    "    mask = torch.rand(*data.shape[:2], # 随机生成mask值，(batch_size, 128, 1, 1)\n",
    "                      *([1] * (len(data.shape) - 2)),\n",
    "                      device=data.device)\n",
    "    # ratio = np.random.beta(1.0, 1.0, size=(data.shape[0], 1, 1, 1))\n",
    "    # ratio = torch.tensor(ratio, device=mask.device).clamp(max=0.5)\n",
    "    ratio = torch.rand(size=(data.shape[0], 1, 1, 1),\n",
    "                       device=mask.device) * (max_r - min_r) + min_r # 随机生成1个阈值 (batch_size, 1, 1, 1)\n",
    "    mask = mask < ratio # mask值低于阈值的，被置零\n",
    "    mask = mask.expand_as(data) # (batch_size, 128, 1, 1) -> (batch_size, 128, 7, 5)\n",
    "    data[mask] = 0.0\n",
    "    return data, ratio\n",
    "\n",
    "\n",
    "def gradient_penalty(model, real, fake):\n",
    "    device = real.device\n",
    "    real = real.data\n",
    "    fake = fake.data\n",
    "    alpha = torch.rand(real.size(0), *([1] * (len(real.shape) - 1))).to(device)\n",
    "    inputs = alpha * real + ((1 - alpha) * fake)\n",
    "    inputs.requires_grad_()\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    gradient = autograd.grad(outputs=outputs,\n",
    "                             inputs=inputs,\n",
    "                             grad_outputs=torch.ones_like(outputs).to(device),\n",
    "                             create_graph=True,\n",
    "                             retain_graph=True,\n",
    "                             only_inputs=True)[0]\n",
    "\n",
    "    gradient = gradient.flatten(1)\n",
    "    return ((gradient.norm(2, dim=1) - 1)**2).mean()\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, g_model, d_model, trainer_kwargs={'max_epochs': 10}):\n",
    "        super().__init__()\n",
    "        self.g_model = g_model.cuda()\n",
    "        self.d_model = d_model.cuda()\n",
    "\n",
    "        self._loss_fn_ce = nn.CrossEntropyLoss()\n",
    "        self._loss_fn_mse = nn.MSELoss()\n",
    "        self._optimizer_g_model = torch.optim.Adam(\n",
    "            g_model.parameters(),\n",
    "            lr=RECEIVED_PARAMS['g_lr'],\n",
    "            weight_decay=RECEIVED_PARAMS['weight_decay'])\n",
    "        self._optimizer_d_model = torch.optim.Adam(\n",
    "            d_model.parameters(),\n",
    "            lr=RECEIVED_PARAMS['d_lr'],\n",
    "            weight_decay=RECEIVED_PARAMS['weight_decay'])\n",
    "\n",
    "        self._trainer_kwargs = trainer_kwargs\n",
    "\n",
    "        eeg_dataset = sub_dataset\n",
    "        train_dataset, val_dataset = train_test_split(eeg_dataset)\n",
    "        train_dataloader = DataLoader(train_dataset,\n",
    "                                      batch_size=64,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=False)\n",
    "\n",
    "        self._train_dataloader = train_dataloader\n",
    "\n",
    "    def _accuracy(self, input, target):  # pylint: disable=redefined-builtin\n",
    "        _, predict = torch.max(input.data, 1)\n",
    "        correct = predict.eq(target.data).cpu().sum().item()\n",
    "        return correct / input.size(0)\n",
    "\n",
    "    def training_step_g_model(self, batch, batch_idx, augment_fn=random_mask):\n",
    "        self._optimizer_g_model.zero_grad()\n",
    "\n",
    "        for p in self.d_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        x, y = batch\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        aug_x, ratio = random_mask(x)\n",
    "        pred_x = self.g_model(aug_x)\n",
    "        loss = -self.d_model(pred_x).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self._optimizer_g_model.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step_d_model(self, batch, batch_idx, augment_fn=random_mask):\n",
    "        self._optimizer_d_model.zero_grad()\n",
    "\n",
    "        for p in self.d_model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        x, y = batch\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        aug_x, ratio = random_mask(x)\n",
    "        pred_x = self.g_model(aug_x).detach()\n",
    "\n",
    "        loss = self.d_model(pred_x).mean() - self.d_model(x).mean()\n",
    "        loss += RECEIVED_PARAMS['weight_gp'] * gradient_penalty(\n",
    "            self.d_model, x, pred_x)\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            loss.backward()\n",
    "            self._optimizer_d_model.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _train(self, epoch_idx=-1):\n",
    "        \"\"\"\n",
    "        单独显示每个 epoch 的训练进度条，并动态更新 G 和 D 的损失。\n",
    "        \"\"\"\n",
    "        pbar = tqdm(total=len(self._train_dataloader), desc=f\"[TRAIN] Epoch {epoch_idx}\")\n",
    "        for i, batch in enumerate(self._train_dataloader):\n",
    "            # 获取 D 模型的损失\n",
    "            loss_d_model = self.training_step_d_model(batch, i)\n",
    "            # 获取 G 模型的损失\n",
    "            loss_g_model = self.training_step_g_model(batch, i)\n",
    "\n",
    "            # 更新进度条\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(\n",
    "                ordered_dict={\n",
    "                    'loss_g_model': f'{loss_g_model.item():.3f}',\n",
    "                    'loss_d_model': f'{loss_d_model.item():.3f}'\n",
    "                }\n",
    "            )\n",
    "        pbar.close()\n",
    "        \n",
    "    def fit(self) -> None:\n",
    "        for i in range(self._trainer_kwargs['max_epochs']):\n",
    "            self._train(i + 1)\n",
    "\n",
    "    def save(self, param_path):\n",
    "        torch.save(\n",
    "            {\n",
    "                'g_model': self.g_model.state_dict(),\n",
    "                'd_model': self.d_model.state_dict()\n",
    "            }, param_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=4):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layer1 = nn.Conv2d(in_channels,\n",
    "                                256,\n",
    "                                kernel_size=3,\n",
    "                                stride=1,\n",
    "                                padding=1,\n",
    "                                bias=True)\n",
    "        self.layer2 = nn.Conv2d(256,\n",
    "                                128,\n",
    "                                kernel_size=5,\n",
    "                                stride=1,\n",
    "                                padding=2,\n",
    "                                bias=True)\n",
    "        self.layer3 = nn.Conv2d(128,\n",
    "                                64,\n",
    "                                kernel_size=5,\n",
    "                                stride=1,\n",
    "                                padding=2,\n",
    "                                bias=True)\n",
    "        self.layer4 = SeparableConv2d(64,\n",
    "                                      32,\n",
    "                                      kernel_size=5,\n",
    "                                      stride=1,\n",
    "                                      padding=2,\n",
    "                                      bias=True)\n",
    "        self.layer5 = InceptionConv2d(32, 16)\n",
    "        self.drop = nn.Sequential(nn.Dropout(), nn.SELU())\n",
    "        self.fc1 = nn.Sequential(nn.Linear(9 * 9 * 16, 1024, bias=True),\n",
    "                                 nn.SELU())\n",
    "        self.fc2 = nn.Linear(1024, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.drop(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        feat = self.fc1(out)\n",
    "        out = self.fc2(feat)\n",
    "        return out, feat\n",
    "\n",
    "\n",
    "c_model = Classifier(num_classes=2, in_channels=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model = SwinTransformer_D(in_chans=128,\n",
    "                            num_classes=2,\n",
    "                            embed_dim=96,\n",
    "                            depths=(2, 2, 6, 2),\n",
    "                            num_heads=(2, 4, 6, 8),\n",
    "                            visual_mode=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTrainer():\n",
    "    def __init__(self, c_model, trainer_kwargs={'max_epochs': 10}):\n",
    "        super().__init__()\n",
    "        self.c_model = c_model.cuda()\n",
    "\n",
    "        self._loss_fn_ce = nn.CrossEntropyLoss()\n",
    "        self._optimizer_c_model = torch.optim.Adam(c_model.parameters(),\n",
    "                                                   lr=RECEIVED_PARAMS['c_lr'],\n",
    "                                                   weight_decay=0.0005)\n",
    "        self._trainer_kwargs = trainer_kwargs\n",
    "\n",
    "        eeg_dataset = sub_dataset\n",
    "        train_dataset, val_dataset = train_test_split(eeg_dataset)\n",
    "        train_dataloader = DataLoader(train_dataset,\n",
    "                                      batch_size=64,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=False)\n",
    "        val_dataloader = DataLoader(val_dataset,\n",
    "                                    batch_size=64,\n",
    "                                    shuffle=False,\n",
    "                                    drop_last=False)\n",
    "\n",
    "        self._train_dataloader = train_dataloader\n",
    "        self._val_dataloader = val_dataloader\n",
    "\n",
    "    def _accuracy(self, input, target):  # pylint: disable=redefined-builtin\n",
    "        _, predict = torch.max(input.data, 1)\n",
    "        correct = predict.eq(target.data).cpu().sum().item()\n",
    "        return correct / input.size(0)\n",
    "\n",
    "    def training_step_c_model(self, batch, batch_idx):\n",
    "        for p in self.c_model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        self._optimizer_c_model.zero_grad()\n",
    "\n",
    "        x, y = batch\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        y_hat, x_feat = self.c_model(x)\n",
    "        loss = self._loss_fn_ce(y_hat, y)\n",
    "\n",
    "        loss.backward()\n",
    "        self._optimizer_c_model.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = self.validation_step_before_model(batch, batch_idx)\n",
    "        y_hat, x_feat = self.c_model(x)\n",
    "        return (y_hat.detach().cpu(), y.detach().cpu())\n",
    "\n",
    "    def validation_step_before_model(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        return x, y\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # We might need dict metrics in future?\n",
    "        y_hat, y = zip(*outputs)\n",
    "        y_hat = torch.cat(y_hat, dim=0)\n",
    "        y = torch.cat(y, dim=0)\n",
    "        avg_acc = self._accuracy(y_hat, y)\n",
    "        print(\"acc:\", avg_acc)\n",
    "        # logger.info('[VAL] Average ACC at epoch end is {}'.format(avg_acc))\n",
    "        # return {'val_acc': avg_acc}\n",
    "\n",
    "    def _validate(self, epoch_idx=-1):\n",
    "        validation_outputs = []\n",
    "        for i, batch in enumerate(self._val_dataloader):\n",
    "            validation_outputs.append(self.validation_step(batch, i))\n",
    "        return self.validation_epoch_end(validation_outputs)\n",
    "\n",
    "    def _train(self, epoch_idx=-1):\n",
    "        \"\"\"\n",
    "        单独显示每个 epoch 的训练进度条。\n",
    "        \"\"\"\n",
    "        pbar = tqdm(total=len(self._train_dataloader), desc=f\"[TRAIN] Epoch {epoch_idx}\")\n",
    "        for i, batch in enumerate(self._train_dataloader):\n",
    "            loss_c_model = self.training_step_c_model(batch, i)\n",
    "            pbar.update(1)\n",
    "            # 更新进度条的后缀信息\n",
    "            pbar.set_postfix(ordered_dict={'loss_c_model': f'{loss_c_model.item():.3f}'})\n",
    "        pbar.close()\n",
    "\n",
    "    def fit(self) -> None:\n",
    "        \"\"\"\n",
    "        按照每个 epoch 单独创建训练和验证进度条。\n",
    "        \"\"\"\n",
    "        for epoch_idx in range(self._trainer_kwargs['max_epochs']):\n",
    "            self._train(epoch_idx + 1)\n",
    "            self._validate(epoch_idx + 1)\n",
    "\n",
    "        # logger.info('[VAL] Final ACC at experiment end is {}'.format(\n",
    "        #     self._validate()['val_acc']))\n",
    "\n",
    "    def save(self, param_path):\n",
    "        torch.save({\n",
    "            'c_model': self.c_model.state_dict(),\n",
    "        }, param_path)\n",
    "\n",
    "\n",
    "trainer = CTrainer(c_model, trainer_kwargs={'max_epochs': 50})\n",
    "trainer.fit()\n",
    "trainer.save('./parameters/' + 'cross_validation_backbone.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SwinTransformer, SwinTransformer_D\n",
    "\n",
    "c_model = SwinTransformer_D(in_chans=128,\n",
    "                            num_classes=2,\n",
    "                            embed_dim=96,\n",
    "                            depths=(2, 2, 6, 2),\n",
    "                            num_heads=(2, 4, 6, 8),\n",
    "                            visual_mode=True\n",
    "                            )\n",
    "d_model = SwinTransformer(in_chans=128,\n",
    "                          num_classes=2,\n",
    "                          embed_dim=96,\n",
    "                          depths=(2, 2, 6, 2),\n",
    "                          num_heads=(2, 4, 6, 8),\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 128, 9, 9])\n",
      "tensor([[-0.4754, -0.5544],\n",
      "        [-0.6015,  0.7165]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 模拟输入\n",
    "import torch\n",
    "input_tensor = torch.randn(2, 128, 9, 9)  # [batch, 1, 128, 7, 5]\n",
    "output = c_model(input_tensor)\n",
    "\n",
    "print(\"Input shape:\", input_tensor.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 1: 100%|██████████| 4/4 [00:00<00:00, 11.95it/s, loss_c_model=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 1] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 2: 100%|██████████| 4/4 [00:00<00:00, 12.10it/s, loss_c_model=0.322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 2] Validation Accuracy: 0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 3: 100%|██████████| 4/4 [00:00<00:00, 11.65it/s, loss_c_model=0.294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 3] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 4: 100%|██████████| 4/4 [00:00<00:00, 11.34it/s, loss_c_model=0.222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 4] Validation Accuracy: 0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 5: 100%|██████████| 4/4 [00:00<00:00, 11.69it/s, loss_c_model=0.162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 5] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 6: 100%|██████████| 4/4 [00:00<00:00, 12.10it/s, loss_c_model=0.164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 6] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 7: 100%|██████████| 4/4 [00:00<00:00, 12.11it/s, loss_c_model=0.180]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 7] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 8: 100%|██████████| 4/4 [00:00<00:00, 12.15it/s, loss_c_model=0.152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 8] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 12.10it/s, loss_c_model=0.142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 9] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 10: 100%|██████████| 4/4 [00:00<00:00, 12.24it/s, loss_c_model=0.095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 10] Validation Accuracy: 0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 11: 100%|██████████| 4/4 [00:00<00:00, 12.08it/s, loss_c_model=0.100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 11] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 12: 100%|██████████| 4/4 [00:00<00:00, 11.99it/s, loss_c_model=0.107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 12] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 13: 100%|██████████| 4/4 [00:00<00:00, 12.06it/s, loss_c_model=0.111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 13] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 14: 100%|██████████| 4/4 [00:00<00:00, 12.20it/s, loss_c_model=0.113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 14] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 15: 100%|██████████| 4/4 [00:00<00:00, 11.69it/s, loss_c_model=0.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 15] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 16: 100%|██████████| 4/4 [00:00<00:00, 12.14it/s, loss_c_model=0.092]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 16] Validation Accuracy: 0.519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 17: 100%|██████████| 4/4 [00:00<00:00, 12.16it/s, loss_c_model=0.085]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 17] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 18: 100%|██████████| 4/4 [00:00<00:00, 12.21it/s, loss_c_model=0.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 18] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 19: 100%|██████████| 4/4 [00:00<00:00, 11.54it/s, loss_c_model=0.065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 19] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 20: 100%|██████████| 4/4 [00:00<00:00, 11.78it/s, loss_c_model=0.092]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 20] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 21: 100%|██████████| 4/4 [00:00<00:00, 12.16it/s, loss_c_model=0.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 21] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 22: 100%|██████████| 4/4 [00:00<00:00, 12.24it/s, loss_c_model=0.076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 22] Validation Accuracy: 0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 23: 100%|██████████| 4/4 [00:00<00:00, 11.66it/s, loss_c_model=0.065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 23] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 24: 100%|██████████| 4/4 [00:00<00:00, 12.17it/s, loss_c_model=0.067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 24] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 25: 100%|██████████| 4/4 [00:00<00:00, 12.19it/s, loss_c_model=0.056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 25] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 26: 100%|██████████| 4/4 [00:00<00:00, 12.12it/s, loss_c_model=0.065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 26] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 27: 100%|██████████| 4/4 [00:00<00:00, 12.42it/s, loss_c_model=0.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 27] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 28: 100%|██████████| 4/4 [00:00<00:00, 12.55it/s, loss_c_model=0.077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 28] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 29: 100%|██████████| 4/4 [00:00<00:00, 12.31it/s, loss_c_model=0.068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 29] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 30: 100%|██████████| 4/4 [00:00<00:00, 12.37it/s, loss_c_model=0.052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 30] Validation Accuracy: 0.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 31: 100%|██████████| 4/4 [00:00<00:00, 12.17it/s, loss_c_model=0.055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 31] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 32: 100%|██████████| 4/4 [00:00<00:00, 12.27it/s, loss_c_model=0.065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 32] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 33: 100%|██████████| 4/4 [00:00<00:00, 12.42it/s, loss_c_model=0.067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 33] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 34: 100%|██████████| 4/4 [00:00<00:00, 12.05it/s, loss_c_model=0.057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 34] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 35: 100%|██████████| 4/4 [00:00<00:00, 12.19it/s, loss_c_model=0.061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 35] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 36: 100%|██████████| 4/4 [00:00<00:00, 12.32it/s, loss_c_model=0.053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 36] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 37: 100%|██████████| 4/4 [00:00<00:00, 12.13it/s, loss_c_model=0.059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 37] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 38: 100%|██████████| 4/4 [00:00<00:00, 12.39it/s, loss_c_model=0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 38] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 39: 100%|██████████| 4/4 [00:00<00:00, 12.12it/s, loss_c_model=0.049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 39] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 40: 100%|██████████| 4/4 [00:00<00:00, 12.31it/s, loss_c_model=0.045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 40] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 41: 100%|██████████| 4/4 [00:00<00:00, 12.39it/s, loss_c_model=0.050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 41] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 42: 100%|██████████| 4/4 [00:00<00:00, 11.94it/s, loss_c_model=0.053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 42] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 43: 100%|██████████| 4/4 [00:00<00:00, 12.34it/s, loss_c_model=0.042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 43] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 44: 100%|██████████| 4/4 [00:00<00:00, 12.04it/s, loss_c_model=0.052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 44] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 45: 100%|██████████| 4/4 [00:00<00:00, 11.59it/s, loss_c_model=0.049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 45] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 46: 100%|██████████| 4/4 [00:00<00:00, 12.26it/s, loss_c_model=0.046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 46] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 47: 100%|██████████| 4/4 [00:00<00:00, 12.20it/s, loss_c_model=0.044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 47] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 48: 100%|██████████| 4/4 [00:00<00:00, 11.91it/s, loss_c_model=0.048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 48] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 49: 100%|██████████| 4/4 [00:00<00:00, 12.14it/s, loss_c_model=0.046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 49] Validation Accuracy: 0.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 50: 100%|██████████| 4/4 [00:00<00:00, 12.04it/s, loss_c_model=0.044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 50] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 51: 100%|██████████| 4/4 [00:00<00:00, 12.14it/s, loss_c_model=0.046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 51] Validation Accuracy: 0.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 52: 100%|██████████| 4/4 [00:00<00:00, 12.24it/s, loss_c_model=0.046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 52] Validation Accuracy: 0.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 53: 100%|██████████| 4/4 [00:00<00:00, 11.80it/s, loss_c_model=0.042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 53] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 54: 100%|██████████| 4/4 [00:00<00:00, 12.02it/s, loss_c_model=0.043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 54] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 55: 100%|██████████| 4/4 [00:00<00:00, 12.32it/s, loss_c_model=0.057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 55] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 56: 100%|██████████| 4/4 [00:00<00:00, 12.11it/s, loss_c_model=0.038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 56] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 57: 100%|██████████| 4/4 [00:00<00:00, 12.46it/s, loss_c_model=0.047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 57] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 58: 100%|██████████| 4/4 [00:00<00:00, 12.49it/s, loss_c_model=0.041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 58] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 59: 100%|██████████| 4/4 [00:00<00:00, 12.60it/s, loss_c_model=0.042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 59] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 60: 100%|██████████| 4/4 [00:00<00:00, 12.51it/s, loss_c_model=0.043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 60] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 61: 100%|██████████| 4/4 [00:00<00:00, 12.49it/s, loss_c_model=0.044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 61] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 62: 100%|██████████| 4/4 [00:00<00:00, 12.41it/s, loss_c_model=0.038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 62] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 63: 100%|██████████| 4/4 [00:00<00:00, 12.39it/s, loss_c_model=0.043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 63] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 64: 100%|██████████| 4/4 [00:00<00:00, 12.46it/s, loss_c_model=0.038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 64] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 65: 100%|██████████| 4/4 [00:00<00:00, 12.32it/s, loss_c_model=0.043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 65] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 66: 100%|██████████| 4/4 [00:00<00:00, 12.40it/s, loss_c_model=0.042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 66] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 67: 100%|██████████| 4/4 [00:00<00:00, 12.57it/s, loss_c_model=0.042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 67] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 68: 100%|██████████| 4/4 [00:00<00:00, 12.56it/s, loss_c_model=0.037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 68] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 69: 100%|██████████| 4/4 [00:00<00:00, 12.53it/s, loss_c_model=0.051]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 69] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 70: 100%|██████████| 4/4 [00:00<00:00, 12.41it/s, loss_c_model=0.036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 70] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 71: 100%|██████████| 4/4 [00:00<00:00, 12.26it/s, loss_c_model=0.042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 71] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 72: 100%|██████████| 4/4 [00:00<00:00, 12.34it/s, loss_c_model=0.037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 72] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 73: 100%|██████████| 4/4 [00:00<00:00, 12.52it/s, loss_c_model=0.041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 73] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 74: 100%|██████████| 4/4 [00:00<00:00, 12.62it/s, loss_c_model=0.039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 74] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 75: 100%|██████████| 4/4 [00:00<00:00, 12.59it/s, loss_c_model=0.035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 75] Validation Accuracy: 0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 76: 100%|██████████| 4/4 [00:00<00:00, 12.21it/s, loss_c_model=0.035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 76] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 77: 100%|██████████| 4/4 [00:00<00:00, 12.44it/s, loss_c_model=0.034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 77] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 78: 100%|██████████| 4/4 [00:00<00:00, 12.15it/s, loss_c_model=0.042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 78] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 79: 100%|██████████| 4/4 [00:00<00:00, 12.43it/s, loss_c_model=0.034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 79] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 80: 100%|██████████| 4/4 [00:00<00:00, 12.43it/s, loss_c_model=0.035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 80] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 81: 100%|██████████| 4/4 [00:00<00:00, 12.09it/s, loss_c_model=0.039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 81] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 82: 100%|██████████| 4/4 [00:00<00:00, 12.31it/s, loss_c_model=0.030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 82] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 83: 100%|██████████| 4/4 [00:00<00:00, 12.22it/s, loss_c_model=0.030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 83] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 84: 100%|██████████| 4/4 [00:00<00:00, 12.04it/s, loss_c_model=0.035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 84] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 85: 100%|██████████| 4/4 [00:00<00:00, 12.32it/s, loss_c_model=0.031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 85] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 86: 100%|██████████| 4/4 [00:00<00:00, 12.48it/s, loss_c_model=0.030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 86] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 87: 100%|██████████| 4/4 [00:00<00:00, 12.28it/s, loss_c_model=0.037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 87] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 88: 100%|██████████| 4/4 [00:00<00:00, 12.30it/s, loss_c_model=0.029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 88] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 89: 100%|██████████| 4/4 [00:00<00:00, 12.44it/s, loss_c_model=0.031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 89] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 90: 100%|██████████| 4/4 [00:00<00:00, 11.83it/s, loss_c_model=0.029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 90] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 91: 100%|██████████| 4/4 [00:00<00:00, 12.49it/s, loss_c_model=0.036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 91] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 92: 100%|██████████| 4/4 [00:00<00:00, 12.48it/s, loss_c_model=0.033]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 92] Validation Accuracy: 0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 93: 100%|██████████| 4/4 [00:00<00:00, 11.75it/s, loss_c_model=0.033]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 93] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 94: 100%|██████████| 4/4 [00:00<00:00, 12.54it/s, loss_c_model=0.029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 94] Validation Accuracy: 0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 95: 100%|██████████| 4/4 [00:00<00:00, 12.48it/s, loss_c_model=0.034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 95] Validation Accuracy: 0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 96: 100%|██████████| 4/4 [00:00<00:00, 12.01it/s, loss_c_model=0.029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 96] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 97: 100%|██████████| 4/4 [00:00<00:00, 12.26it/s, loss_c_model=0.030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 97] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 98: 100%|██████████| 4/4 [00:00<00:00, 11.49it/s, loss_c_model=0.031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 98] Validation Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 99: 100%|██████████| 4/4 [00:00<00:00, 10.56it/s, loss_c_model=0.035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 99] Validation Accuracy: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[TRAIN] Epoch 100: 100%|██████████| 4/4 [00:00<00:00, 12.08it/s, loss_c_model=0.031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 100] Validation Accuracy: 0.596\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCTrainer():\n",
    "    def __init__(self, c_model, g_model, trainer_kwargs={'max_epochs': 10}):\n",
    "        super().__init__()\n",
    "        self.c_model = c_model.cuda()\n",
    "        self.g_model = g_model.cuda()\n",
    "\n",
    "        self._loss_fn_ce = nn.CrossEntropyLoss()\n",
    "        self._loss_fn_mse = nn.MSELoss()\n",
    "        self._optimizer_c_model = torch.optim.Adam(c_model.parameters(),\n",
    "                                                   lr=RECEIVED_PARAMS['c_lr'],\n",
    "                                                   weight_decay=0.0005)\n",
    "\n",
    "        self._trainer_kwargs = trainer_kwargs\n",
    "\n",
    "        eeg_dataset = sub_dataset\n",
    "        train_dataset, val_dataset = train_test_split(eeg_dataset)\n",
    "        train_dataloader = DataLoader(train_dataset,\n",
    "                                      batch_size=64,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=False)\n",
    "        val_dataloader = DataLoader(val_dataset,\n",
    "                                    batch_size=64,\n",
    "                                    shuffle=False,\n",
    "                                    drop_last=False)\n",
    "\n",
    "        self._train_dataloader = train_dataloader\n",
    "        self._val_dataloader = val_dataloader\n",
    "\n",
    "    def _accuracy(self, input, target):  # pylint: disable=redefined-builtin\n",
    "        _, predict = torch.max(input.data, 1)\n",
    "        correct = predict.eq(target.data).cpu().sum().item()\n",
    "        return correct / input.size(0)\n",
    "\n",
    "    def training_step_c_model(self, batch, batch_idx):\n",
    "        for p in self.c_model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        self._optimizer_c_model.zero_grad()\n",
    "\n",
    "        x, y = batch\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        y_hat, x_feat = self.c_model(x)\n",
    "        loss = self._loss_fn_ce(y_hat, y)\n",
    "\n",
    "        aug_x, ratio = random_mask(x)\n",
    "        aug_x = self.g_model(aug_x).detach()\n",
    "        aug_y_hat, aug_x_feat = self.c_model(aug_x)\n",
    "\n",
    "        loss += RECEIVED_PARAMS['weight_ssl'] * (\n",
    "            (1 - ratio).squeeze() * F.mse_loss(\n",
    "                x_feat, aug_x_feat, reduction='none').mean(dim=-1)).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self._optimizer_c_model.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = self.validation_step_before_model(batch, batch_idx)\n",
    "        y_hat, x_feat = self.c_model(x)\n",
    "        return (y_hat.detach().cpu(), y.detach().cpu())\n",
    "\n",
    "    def validation_step_before_model(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        return x, y\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # We might need dict metrics in future?\n",
    "        y_hat, y = zip(*outputs)\n",
    "        y_hat = torch.cat(y_hat, dim=0)\n",
    "        y = torch.cat(y, dim=0)\n",
    "        avg_acc = self._accuracy(y_hat, y)\n",
    "        # logger.info('[VAL] Average ACC at epoch end is {}'.format(avg_acc))\n",
    "        return {'val_acc': avg_acc}\n",
    "\n",
    "    def _validate(self, epoch_idx=-1):\n",
    "        validation_outputs = []\n",
    "        for i, batch in enumerate(self._val_dataloader):\n",
    "            validation_outputs.append(self.validation_step(batch, i))\n",
    "        return self.validation_epoch_end(validation_outputs)\n",
    "\n",
    "    def _train(self, epoch_idx=-1):\n",
    "        \"\"\"\n",
    "        单独显示每个 epoch 的训练进度条。\n",
    "        \"\"\"\n",
    "        pbar = tqdm(total=len(self._train_dataloader), desc=f\"[TRAIN] Epoch {epoch_idx}\")\n",
    "        for i, batch in enumerate(self._train_dataloader):\n",
    "            loss_c_model = self.training_step_c_model(batch, i)\n",
    "            pbar.update(1)\n",
    "            # 更新进度条的后缀信息\n",
    "            pbar.set_postfix(ordered_dict={'loss_c_model': f'{loss_c_model.item():.3f}'})\n",
    "        pbar.close()\n",
    "\n",
    "    def fit(self) -> None:\n",
    "        \"\"\"\n",
    "        按照每个 epoch 单独创建训练和验证进度条。\n",
    "        \"\"\"\n",
    "        for epoch_idx in range(self._trainer_kwargs['max_epochs']):\n",
    "            self._train(epoch_idx + 1)\n",
    "                # 验证过程并获取验证结果\n",
    "            val_metrics = self._validate(epoch_idx + 1)\n",
    "            val_acc = val_metrics['val_acc']\n",
    "            \n",
    "            # 打印验证准确率\n",
    "            print(f\"[EPOCH {epoch_idx + 1}] Validation Accuracy: {val_acc:.3f}\")\n",
    "\n",
    "    def save(self, param_path):\n",
    "        torch.save({\n",
    "            'c_model': trainer.c_model.state_dict(),\n",
    "        }, param_path)\n",
    "\n",
    "    def load(self):\n",
    "        gan_model_state_dict = torch.load(\n",
    "            './parameters/cross_validation_proposed_pretrain.pth')\n",
    "        self.g_model.load_state_dict(gan_model_state_dict['g_model'])\n",
    "\n",
    "        if os.path.exists('./parameters/cross_validation_backbone' + '.pth'):\n",
    "            c_model_state_dict = torch.load(\n",
    "                './parameters/cross_validation_backbone'  +\n",
    "                '.pth')\n",
    "            self.c_model.load_state_dict(c_model_state_dict['c_model'])\n",
    "\n",
    "\n",
    "trainer = GCTrainer(c_model,\n",
    "                  g_model,\n",
    "                  trainer_kwargs={'max_epochs': 100})\n",
    "trainer.load()\n",
    "trainer.fit()\n",
    "trainer.save('./parameters/' +  'cross_validation_finetune.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (layer1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (layer2): Conv2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (layer3): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (layer4): SeparableConv2d(\n",
      "    (depth): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64)\n",
      "    (point): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (layer5): InceptionConv2d(\n",
      "    (conv5x5): Conv2d(32, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (conv3x3): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv1x1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (drop): Sequential(\n",
      "    (0): SELU()\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=560, out_features=1024, bias=True)\n",
      "    (1): SELU()\n",
      "  )\n",
      "  (fc2): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = Generator(in_channels=128, out_channels=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(g_model,\n",
    "                  d_model,\n",
    "                  trainer_kwargs={'max_epochs': 200})\n",
    "trainer.fit()\n",
    "trainer.save('./parameters/' + 'cross_validation_proposed_pretrain2.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "law",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
