{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/conda/envs/law/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch import autograd\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "\n",
    "from strokes import StrokePatientsMIDataset\n",
    "from strokesdict import STROKEPATIENTSMI_LOCATION_DICT\n",
    "import scipy\n",
    "from torcheeg.transforms import Select,BandSignal,Compose,ToTensor\n",
    "from to import ToGrid\n",
    "from typing import Callable, Dict, Union, List\n",
    "import numpy as np\n",
    "import soxr\n",
    "from downsample import SetSamplingRate\n",
    "from baseline import BaselineCorrection\n",
    "from torcheeg.transforms import EEGTransform, Select,BandSignal,Compose,ToTensor\n",
    "\n",
    "dataset = StrokePatientsMIDataset(root_path='../../mi_swin/subdataset',\n",
    "                                  io_path='.torcheeg/datasets_1741312808642_RKI0H',\n",
    "                        chunk_size=500,  # 1 second\n",
    "                        overlap = 0,\n",
    "                        offline_transform=Compose(\n",
    "                                [BaselineCorrection(),\n",
    "                                SetSamplingRate(origin_sampling_rate=500,target_sampling_rate=128),\n",
    "                                BandSignal(sampling_rate=128,band_dict={'frequency_range':[8,40]})\n",
    "                                ]),\n",
    "                        online_transform=Compose(\n",
    "                                # [ToTensor()]),\n",
    "                                [ToGrid(STROKEPATIENTSMI_LOCATION_DICT),ToTensor()]),\n",
    "                \n",
    "                        label_transform=Select('label'),\n",
    "                        num_worker=8\n",
    ")\n",
    "print(dataset[0][0].shape) #EEG shape(1,30,128)\n",
    "print(dataset[0][1])  # label (int)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def delete_folder_if_exists(target_folder_name):\n",
    "    # 获取父文件夹中的所有内容\n",
    "    parent_folder = os.getcwd()\n",
    "    for folder_name in os.listdir(parent_folder):\n",
    "        folder_path = os.path.join(parent_folder, folder_name)\n",
    "\n",
    "        # 检查是否是文件夹并且名称是否匹配\n",
    "        if os.path.isdir(folder_path) and folder_name == target_folder_name:\n",
    "            try:\n",
    "                # 删除目标文件夹\n",
    "                shutil.rmtree(folder_path)\n",
    "                print(f\"已删除文件夹: {folder_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"删除文件夹 {folder_path} 时出错: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, test_size=0.2, random_state=520, shuffle=True):\n",
    "    n_samples = len(dataset)\n",
    "    indices = np.arange(n_samples)\n",
    "    train_index, test_index = model_selection.train_test_split(\n",
    "        indices,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        shuffle=shuffle)\n",
    "\n",
    "    trian_dataset = Subset(dataset, train_index)\n",
    "    test_dataset = Subset(dataset, test_index)\n",
    "\n",
    "    return trian_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dataset, test_dataset = train_test_split(dataset=dataset)\n",
    "print(len(sub_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECEIVED_PARAMS = {\n",
    "    \"c_lr\": 0.00001,\n",
    "    \"g_lr\": 0.00001,\n",
    "    \"d_lr\": 0.00001,\n",
    "    \"weight_gp\": 1.0,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"weight_ssl\": 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=128, out_channels=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,\n",
    "                      128,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=True), nn.LeakyReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            nn.LeakyReLU())\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            nn.LeakyReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.LeakyReLU())\n",
    "        self.delayer1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16 + 32,\n",
    "                               32,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=True), nn.LeakyReLU())\n",
    "        self.delayer2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32 + 64,\n",
    "                               64,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=True), nn.LeakyReLU())\n",
    "        self.delayer3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64 + 128,\n",
    "                               128,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #         x = channel_to_location(x)\n",
    "        mask = (x.abs().sum(dim=1, keepdim=True) > 0).float()\n",
    "        out1 = self.layer1(x)\n",
    "        out2 = self.layer2(out1)\n",
    "        out3 = self.layer3(out2)\n",
    "        out = self.layer4(out3)\n",
    "        out = self.delayer1(torch.cat([out, out3], dim=1))\n",
    "        out = self.delayer2(torch.cat([out, out2], dim=1))\n",
    "        out = self.delayer3(torch.cat([out, out1], dim=1))\n",
    "\n",
    "        return out * mask\n",
    "\n",
    "\n",
    "class ResidualConv2d(nn.Module):  # 貌似并未使用该函数\n",
    "    def __init__(self, in_channels, out_channels, bias=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,\n",
    "                      out_channels,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=bias), nn.SELU(),\n",
    "            nn.Conv2d(out_channels,\n",
    "                      out_channels,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=bias))\n",
    "        self.res = nn.Conv2d(in_channels,\n",
    "                             out_channels,\n",
    "                             kernel_size=1,\n",
    "                             stride=1,\n",
    "                             padding=0,\n",
    "                             bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x) + self.res(x)\n",
    "\n",
    "# 识别情感需要分析不同空间尺度下的EEG信号，故引入了包含三种不同尺寸滤波器的InceptionConv2d来提取多尺度特征图\n",
    "class InceptionConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True):\n",
    "        super().__init__()\n",
    "        self.conv5x5 = nn.Conv2d(in_channels,\n",
    "                                 out_channels,\n",
    "                                 kernel_size=5,\n",
    "                                 stride=1,\n",
    "                                 padding=2,\n",
    "                                 bias=bias)\n",
    "        self.conv3x3 = nn.Conv2d(in_channels,\n",
    "                                 out_channels,\n",
    "                                 kernel_size=3,\n",
    "                                 stride=1,\n",
    "                                 padding=1,\n",
    "                                 bias=bias)\n",
    "        self.conv1x1 = nn.Conv2d(in_channels,\n",
    "                                 out_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0,\n",
    "                                 bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv5x5(x) + self.conv3x3(x) + self.conv1x1(x)\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 padding=1,\n",
    "                 bias=True):\n",
    "        super().__init__()\n",
    "        # 不同时间点的二维数据分别进行卷积\n",
    "        self.depth = nn.Conv2d(in_channels,\n",
    "                               in_channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=stride,\n",
    "                               padding=padding,\n",
    "                               groups=in_channels,\n",
    "                               bias=bias)\n",
    "        # 单个eeg通道跨时间点进行卷积\n",
    "        self.point = nn.Conv2d(in_channels,\n",
    "                               out_channels,\n",
    "                               kernel_size=1,\n",
    "                               stride=stride,\n",
    "                               padding=0,\n",
    "                               bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depth(x)\n",
    "        x = self.point(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=4):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layer1 = nn.Conv2d(in_channels,\n",
    "                                256,\n",
    "                                kernel_size=3,\n",
    "                                stride=1,\n",
    "                                padding=1,\n",
    "                                bias=True)\n",
    "        self.layer2 = nn.Conv2d(256,\n",
    "                                128,\n",
    "                                kernel_size=5,\n",
    "                                stride=1,\n",
    "                                padding=2,\n",
    "                                bias=True)\n",
    "        self.layer3 = nn.Conv2d(128,\n",
    "                                64,\n",
    "                                kernel_size=5,\n",
    "                                stride=1,\n",
    "                                padding=2,\n",
    "                                bias=True)\n",
    "        self.layer4 = SeparableConv2d(64,\n",
    "                                      32,\n",
    "                                      kernel_size=5,\n",
    "                                      stride=1,\n",
    "                                      padding=2,\n",
    "                                      bias=True)\n",
    "        self.layer5 = InceptionConv2d(32, 16)\n",
    "\n",
    "        self.drop = nn.Sequential(nn.SELU())\n",
    "        self.fc1 = nn.Sequential(nn.Linear(9 * 9 * 16, 1024, bias=True),\n",
    "                                 nn.SELU()) # stroke MI dataset 的网格为 7*5\n",
    "        self.fc2 = nn.Linear(1024, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.drop(out)\n",
    "        out = out.view(out.size(0), -1) # (batch_size, num_features)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SwinTransformerGenerator\n",
    "import torch\n",
    "# Instantiate the generator model\n",
    "g_model = SwinTransformerGenerator(in_chans=128,\n",
    "                                     patch_size=2,\n",
    "                                     window_size=3,\n",
    "                                     embed_dim=96,\n",
    "                                     depths=(2, 2, 4, 2),\n",
    "                                     num_heads=(2, 2, 4, 6)\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 9, 9])\n",
      "Output shape: torch.Size([2, 1, 128, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "g_model = Generator(in_channels=128, out_channels=128)\n",
    "# d_model = Discriminator(in_channels=128, num_classes=2)\n",
    "# 模拟输入\n",
    "input_tensor = torch.randn(2,1, 128, 9, 9)  # [batch, 1, 128, 7, 5]\n",
    "output = g_model(input_tensor)\n",
    "# ans = d_model(output)\n",
    "print(\"Output shape:\", output.shape)\n",
    "# print(\"ans shape:\", ans.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mask(data, min_r=0.0, max_r=0.5):\n",
    "    # batch_size*channel_num*time_step\n",
    "    data = data.clone()\n",
    "    mask = torch.rand(*data.shape[:2], # 随机生成mask值，(batch_size, 128, 1, 1)\n",
    "                      *([1] * (len(data.shape) - 2)),\n",
    "                      device=data.device)\n",
    "    # ratio = np.random.beta(1.0, 1.0, size=(data.shape[0], 1, 1, 1))\n",
    "    # ratio = torch.tensor(ratio, device=mask.device).clamp(max=0.5)\n",
    "    ratio = torch.rand(size=(data.shape[0], 1, 1, 1),\n",
    "                       device=mask.device) * (max_r - min_r) + min_r # 随机生成1个阈值 (batch_size, 1, 1, 1)\n",
    "    mask = mask < ratio # mask值低于阈值的，被置零\n",
    "    mask = mask.expand_as(data) # (batch_size, 128, 1, 1) -> (batch_size, 128, 7, 5)\n",
    "    data[mask] = 0.0\n",
    "    return data, ratio\n",
    "\n",
    "\n",
    "def gradient_penalty(model, real, fake):\n",
    "    device = real.device\n",
    "    real = real.data\n",
    "    fake = fake.data\n",
    "    alpha = torch.rand(real.size(0), *([1] * (len(real.shape) - 1))).to(device)\n",
    "    inputs = alpha * real + ((1 - alpha) * fake)\n",
    "    inputs.requires_grad_()\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    gradient = autograd.grad(outputs=outputs,\n",
    "                             inputs=inputs,\n",
    "                             grad_outputs=torch.ones_like(outputs).to(device),\n",
    "                             create_graph=True,\n",
    "                             retain_graph=True,\n",
    "                             only_inputs=True)[0]\n",
    "\n",
    "    gradient = gradient.flatten(1)\n",
    "    return ((gradient.norm(2, dim=1) - 1)**2).mean()\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, g_model, d_model, trainer_kwargs={'max_epochs': 10}):\n",
    "        super().__init__()\n",
    "        self.g_model = g_model.cuda()\n",
    "        self.d_model = d_model.cuda()\n",
    "\n",
    "        self._loss_fn_ce = nn.CrossEntropyLoss()\n",
    "        self._loss_fn_mse = nn.MSELoss()\n",
    "        self._optimizer_g_model = torch.optim.Adam(\n",
    "            g_model.parameters(),\n",
    "            lr=RECEIVED_PARAMS['g_lr'],\n",
    "            weight_decay=RECEIVED_PARAMS['weight_decay'])\n",
    "        self._optimizer_d_model = torch.optim.Adam(\n",
    "            d_model.parameters(),\n",
    "            lr=RECEIVED_PARAMS['d_lr'],\n",
    "            weight_decay=RECEIVED_PARAMS['weight_decay'])\n",
    "\n",
    "        self._trainer_kwargs = trainer_kwargs\n",
    "\n",
    "        eeg_dataset = dataset\n",
    "        train_dataset, val_dataset = train_test_split(eeg_dataset)\n",
    "        train_dataloader = DataLoader(train_dataset,\n",
    "                                      batch_size=16,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=False)\n",
    "\n",
    "        self._train_dataloader = train_dataloader\n",
    "\n",
    "    def _accuracy(self, input, target):  # pylint: disable=redefined-builtin\n",
    "        _, predict = torch.max(input.data, 1)\n",
    "        correct = predict.eq(target.data).cpu().sum().item()\n",
    "        return correct / input.size(0)\n",
    "\n",
    "    def training_step_g_model(self, batch, batch_idx, augment_fn=random_mask):\n",
    "        self._optimizer_g_model.zero_grad()\n",
    "\n",
    "        for p in self.d_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        x, y = batch\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        aug_x, ratio = random_mask(x)\n",
    "        pred_x = self.g_model(aug_x)\n",
    "        loss = -self.d_model(pred_x).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self._optimizer_g_model.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step_d_model(self, batch, batch_idx, augment_fn=random_mask):\n",
    "        self._optimizer_d_model.zero_grad()\n",
    "\n",
    "        for p in self.d_model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        x, y = batch\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        aug_x, ratio = random_mask(x)\n",
    "        pred_x = self.g_model(aug_x).detach()\n",
    "\n",
    "        loss = self.d_model(pred_x).mean() - self.d_model(x).mean()\n",
    "        loss += RECEIVED_PARAMS['weight_gp'] * gradient_penalty(\n",
    "            self.d_model, x, pred_x)\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            loss.backward()\n",
    "            self._optimizer_d_model.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _train(self, epoch_idx=-1):\n",
    "        \"\"\"\n",
    "        单独显示每个 epoch 的训练进度条，并动态更新 G 和 D 的损失。\n",
    "        \"\"\"\n",
    "        pbar = tqdm(total=len(self._train_dataloader), desc=f\"[TRAIN] Epoch {epoch_idx}\")\n",
    "        for i, batch in enumerate(self._train_dataloader):\n",
    "            # 获取 D 模型的损失\n",
    "            loss_d_model = self.training_step_d_model(batch, i)\n",
    "            # 获取 G 模型的损失\n",
    "            loss_g_model = self.training_step_g_model(batch, i)\n",
    "\n",
    "            # 更新进度条\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(\n",
    "                ordered_dict={\n",
    "                    'loss_g_model': f'{loss_g_model.item():.3f}',\n",
    "                    'loss_d_model': f'{loss_d_model.item():.3f}'\n",
    "                }\n",
    "            )\n",
    "        pbar.close()\n",
    "        \n",
    "    def fit(self) -> None:\n",
    "        for i in range(self._trainer_kwargs['max_epochs']):\n",
    "            self._train(i + 1)\n",
    "\n",
    "    def save(self, param_path):\n",
    "        torch.save(\n",
    "            {\n",
    "                'g_model': self.g_model.state_dict(),\n",
    "                'd_model': self.d_model.state_dict()\n",
    "            }, param_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=4):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layer1 = nn.Conv2d(in_channels,\n",
    "                                256,\n",
    "                                kernel_size=3,\n",
    "                                stride=1,\n",
    "                                padding=1,\n",
    "                                bias=True)\n",
    "        self.layer2 = nn.Conv2d(256,\n",
    "                                128,\n",
    "                                kernel_size=5,\n",
    "                                stride=1,\n",
    "                                padding=2,\n",
    "                                bias=True)\n",
    "        self.layer3 = nn.Conv2d(128,\n",
    "                                64,\n",
    "                                kernel_size=5,\n",
    "                                stride=1,\n",
    "                                padding=2,\n",
    "                                bias=True)\n",
    "        self.layer4 = SeparableConv2d(64,\n",
    "                                      32,\n",
    "                                      kernel_size=5,\n",
    "                                      stride=1,\n",
    "                                      padding=2,\n",
    "                                      bias=True)\n",
    "        self.layer5 = InceptionConv2d(32, 16)\n",
    "        self.drop = nn.Sequential(nn.Dropout(), nn.SELU())\n",
    "        self.fc1 = nn.Sequential(nn.Linear(9 * 9 * 16, 1024, bias=True),\n",
    "                                 nn.SELU())\n",
    "        self.fc2 = nn.Linear(1024, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.drop(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        feat = self.fc1(out)\n",
    "        out = self.fc2(feat)\n",
    "        return out, feat\n",
    "\n",
    "\n",
    "c_model = Classifier(num_classes=2, in_channels=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SwinTransformer_D\n",
    "c_model = SwinTransformer_D(in_chans=128,\n",
    "                            num_classes=2,\n",
    "                            embed_dim=96,\n",
    "                            depths=(2, 2, 4, 2),\n",
    "                            num_heads=(2, 2, 4, 6),\n",
    "                            visual_mode=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier import  ClassifierTrainer\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(dataset)\n",
    "trainer = ClassifierTrainer(model=c_model,\n",
    "                            num_classes=2,\n",
    "                            lr=RECEIVED_PARAMS['c_lr'],\n",
    "                            weight_decay=1e-5,\n",
    "                            metrics=[\"accuracy\"],\n",
    "                            accelerator=\"gpu\")\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=16,\n",
    "                              shuffle=True,\n",
    "                              drop_last=False)\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            batch_size=16,\n",
    "                            shuffle=False,\n",
    "                            drop_last=False)\n",
    "\n",
    "trainer.fit(train_dataloader,\n",
    "            val_dataloader,\n",
    "            max_epochs=300,\n",
    "            enable_model_summary=False,\n",
    "            limit_val_batches=0.0)\n",
    "trainer.save('./parameters/' + 'cross_validation_backbone.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            batch_size=16,\n",
    "                            shuffle=False,\n",
    "                            drop_last=False)\n",
    "\n",
    "test_result = trainer.test(val_dataloader,\n",
    "                            enable_progress_bar=True,\n",
    "                            enable_model_summary=True)[0]\n",
    "# training_metrics.append(training_result[\"test_accuracy\"])\n",
    "print(test_result[\"test_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTrainer():\n",
    "    def __init__(self, c_model, trainer_kwargs={'max_epochs': 10}):\n",
    "        super().__init__()\n",
    "        self.c_model = c_model.cuda()\n",
    "\n",
    "        self._loss_fn_ce = nn.CrossEntropyLoss()\n",
    "        self._optimizer_c_model = torch.optim.Adam(c_model.parameters(),\n",
    "                                                   lr=RECEIVED_PARAMS['c_lr'],\n",
    "                                                   weight_decay=0.0005)\n",
    "        self._trainer_kwargs = trainer_kwargs\n",
    "\n",
    "        eeg_dataset = dataset\n",
    "        train_dataset, val_dataset = train_test_split(eeg_dataset)\n",
    "        train_dataloader = DataLoader(train_dataset,\n",
    "                                      batch_size=16,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=False)\n",
    "        val_dataloader = DataLoader(val_dataset,\n",
    "                                    batch_size=16,\n",
    "                                    shuffle=False,\n",
    "                                    drop_last=False)\n",
    "\n",
    "        self._train_dataloader = train_dataloader\n",
    "        self._val_dataloader = val_dataloader\n",
    "\n",
    "    def _accuracy(self, input, target):  # pylint: disable=redefined-builtin\n",
    "        _, predict = torch.max(input.data, 1)\n",
    "        correct = predict.eq(target.data).cpu().sum().item()\n",
    "        return correct / input.size(0)\n",
    "\n",
    "    def training_step_c_model(self, batch, batch_idx):\n",
    "        for p in self.c_model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        self._optimizer_c_model.zero_grad()\n",
    "\n",
    "        x, y = batch\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        y_hat, x_feat = self.c_model(x)\n",
    "        loss = self._loss_fn_ce(y_hat, y)\n",
    "\n",
    "        loss.backward()\n",
    "        self._optimizer_c_model.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = self.validation_step_before_model(batch, batch_idx)\n",
    "        y_hat, x_feat = self.c_model(x)\n",
    "        return (y_hat.detach().cpu(), y.detach().cpu())\n",
    "\n",
    "    def validation_step_before_model(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        return x, y\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # We might need dict metrics in future?\n",
    "        y_hat, y = zip(*outputs)\n",
    "        y_hat = torch.cat(y_hat, dim=0)\n",
    "        y = torch.cat(y, dim=0)\n",
    "        avg_acc = self._accuracy(y_hat, y)\n",
    "        print(\"acc:\", avg_acc)\n",
    "        # logger.info('[VAL] Average ACC at epoch end is {}'.format(avg_acc))\n",
    "        # return {'val_acc': avg_acc}\n",
    "\n",
    "    def _validate(self, epoch_idx=-1):\n",
    "        validation_outputs = []\n",
    "        for i, batch in enumerate(self._val_dataloader):\n",
    "            validation_outputs.append(self.validation_step(batch, i))\n",
    "        return self.validation_epoch_end(validation_outputs)\n",
    "\n",
    "    def _train(self, epoch_idx=-1):\n",
    "        \"\"\"\n",
    "        单独显示每个 epoch 的训练进度条。\n",
    "        \"\"\"\n",
    "        pbar = tqdm(total=len(self._train_dataloader), desc=f\"[TRAIN] Epoch {epoch_idx}\")\n",
    "        for i, batch in enumerate(self._train_dataloader):\n",
    "            loss_c_model = self.training_step_c_model(batch, i)\n",
    "            pbar.update(1)\n",
    "            # 更新进度条的后缀信息\n",
    "            pbar.set_postfix(ordered_dict={'loss_c_model': f'{loss_c_model.item():.3f}'})\n",
    "        pbar.close()\n",
    "\n",
    "    def fit(self) -> None:\n",
    "        \"\"\"\n",
    "        按照每个 epoch 单独创建训练和验证进度条。\n",
    "        \"\"\"\n",
    "        for epoch_idx in range(self._trainer_kwargs['max_epochs']):\n",
    "            self._train(epoch_idx + 1)\n",
    "            self._validate(epoch_idx + 1)\n",
    "\n",
    "        # logger.info('[VAL] Final ACC at experiment end is {}'.format(\n",
    "        #     self._validate()['val_acc']))\n",
    "\n",
    "    def save(self, param_path):\n",
    "        torch.save({\n",
    "            'c_model': self.c_model.state_dict(),\n",
    "        }, param_path)\n",
    "\n",
    "\n",
    "trainer = CTrainer(c_model, trainer_kwargs={'max_epochs': 80})\n",
    "trainer.fit()\n",
    "trainer.save('./parameters/' + 'cross_validation_backbone.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SwinTransformer, SwinTransformer_D\n",
    "\n",
    "c_model = SwinTransformer_D(in_chans=128,\n",
    "                            num_classes=2,\n",
    "                            embed_dim=96,\n",
    "                            depths=(2, 2, 4, 2),\n",
    "                            num_heads=(2, 2, 4, 6),\n",
    "                            visual_mode=True\n",
    "                            )\n",
    "d_model = SwinTransformer(in_chans=128,\n",
    "                          num_classes=2,\n",
    "                          embed_dim=96,\n",
    "                          depths=(2, 2, 4, 2),\n",
    "                          num_heads=(2, 2, 4, 6),\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟输入\n",
    "import torch\n",
    "input_tensor = torch.randn(2, 128, 9, 9)  # [batch, 1, 128, 7, 5]\n",
    "output = d_model(input_tensor)\n",
    "\n",
    "print(\"Input shape:\", input_tensor.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCTrainer():\n",
    "    def __init__(self, c_model, g_model, trainer_kwargs={'max_epochs': 10}):\n",
    "        super().__init__()\n",
    "        self.c_model = c_model.cuda()\n",
    "        self.g_model = g_model.cuda()\n",
    "\n",
    "        self._loss_fn_ce = nn.CrossEntropyLoss()\n",
    "        self._loss_fn_mse = nn.MSELoss()\n",
    "        self._optimizer_c_model = torch.optim.Adam(c_model.parameters(),\n",
    "                                                   lr=RECEIVED_PARAMS['c_lr'],\n",
    "                                                   weight_decay=0.0005)\n",
    "\n",
    "        self._trainer_kwargs = trainer_kwargs\n",
    "\n",
    "        eeg_dataset = dataset\n",
    "        train_dataset, val_dataset = train_test_split(eeg_dataset)\n",
    "        train_dataloader = DataLoader(train_dataset,\n",
    "                                      batch_size=16,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=False)\n",
    "        val_dataloader = DataLoader(val_dataset,\n",
    "                                    batch_size=16,\n",
    "                                    shuffle=False,\n",
    "                                    drop_last=False)\n",
    "\n",
    "        self._train_dataloader = train_dataloader\n",
    "        self._val_dataloader = val_dataloader\n",
    "\n",
    "    def _accuracy(self, input, target):  # pylint: disable=redefined-builtin\n",
    "        _, predict = torch.max(input.data, 1)\n",
    "        correct = predict.eq(target.data).cpu().sum().item()\n",
    "        return correct / input.size(0)\n",
    "\n",
    "    def training_step_c_model(self, batch, batch_idx):\n",
    "        for p in self.c_model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        self._optimizer_c_model.zero_grad()\n",
    "\n",
    "        x, y = batch\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        y_hat, x_feat = self.c_model(x)\n",
    "        loss = self._loss_fn_ce(y_hat, y)\n",
    "\n",
    "        aug_x, ratio = random_mask(x)\n",
    "        aug_x = self.g_model(aug_x).detach()\n",
    "        aug_y_hat, aug_x_feat = self.c_model(aug_x)\n",
    "\n",
    "        loss += RECEIVED_PARAMS['weight_ssl'] * (\n",
    "            (1 - ratio).squeeze() * F.mse_loss(\n",
    "                x_feat, aug_x_feat, reduction='none').mean(dim=-1)).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self._optimizer_c_model.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = self.validation_step_before_model(batch, batch_idx)\n",
    "        y_hat, x_feat = self.c_model(x)\n",
    "        return (y_hat.detach().cpu(), y.detach().cpu())\n",
    "\n",
    "    def validation_step_before_model(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        return x, y\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # We might need dict metrics in future?\n",
    "        y_hat, y = zip(*outputs)\n",
    "        y_hat = torch.cat(y_hat, dim=0)\n",
    "        y = torch.cat(y, dim=0)\n",
    "        avg_acc = self._accuracy(y_hat, y)\n",
    "        # logger.info('[VAL] Average ACC at epoch end is {}'.format(avg_acc))\n",
    "        return {'val_acc': avg_acc}\n",
    "\n",
    "    def _validate(self, epoch_idx=-1):\n",
    "        validation_outputs = []\n",
    "        for i, batch in enumerate(self._val_dataloader):\n",
    "            validation_outputs.append(self.validation_step(batch, i))\n",
    "        return self.validation_epoch_end(validation_outputs)\n",
    "\n",
    "    def _train(self, epoch_idx=-1):\n",
    "        \"\"\"\n",
    "        单独显示每个 epoch 的训练进度条。\n",
    "        \"\"\"\n",
    "        pbar = tqdm(total=len(self._train_dataloader), desc=f\"[TRAIN] Epoch {epoch_idx}\")\n",
    "        for i, batch in enumerate(self._train_dataloader):\n",
    "            loss_c_model = self.training_step_c_model(batch, i)\n",
    "            pbar.update(1)\n",
    "            # 更新进度条的后缀信息\n",
    "            pbar.set_postfix(ordered_dict={'loss_c_model': f'{loss_c_model.item():.3f}'})\n",
    "        pbar.close()\n",
    "\n",
    "    def fit(self) -> None:\n",
    "        \"\"\"\n",
    "        按照每个 epoch 单独创建训练和验证进度条。\n",
    "        \"\"\"\n",
    "        for epoch_idx in range(self._trainer_kwargs['max_epochs']):\n",
    "            self._train(epoch_idx + 1)\n",
    "                # 验证过程并获取验证结果\n",
    "            val_metrics = self._validate(epoch_idx + 1)\n",
    "            val_acc = val_metrics['val_acc']\n",
    "            \n",
    "            # 打印验证准确率\n",
    "            print(f\"[EPOCH {epoch_idx + 1}] Validation Accuracy: {val_acc:.3f}\")\n",
    "\n",
    "    def save(self, param_path):\n",
    "        torch.save({\n",
    "            'c_model': trainer.c_model.state_dict(),\n",
    "        }, param_path)\n",
    "\n",
    "    def load(self):\n",
    "        gan_model_state_dict = torch.load(\n",
    "            './parameters/cross_validation_proposed_pretrain.pth')\n",
    "        self.g_model.load_state_dict(gan_model_state_dict['g_model'])\n",
    "\n",
    "        if os.path.exists('./parameters/cross_validation_backbone' + '.pth'):\n",
    "            c_model_state_dict = torch.load(\n",
    "                './parameters/cross_validation_backbone'  +\n",
    "                '.pth')\n",
    "            self.c_model.load_state_dict(c_model_state_dict['c_model'])\n",
    "\n",
    "\n",
    "trainer = GCTrainer(c_model,\n",
    "                  g_model,\n",
    "                  trainer_kwargs={'max_epochs': 100})\n",
    "trainer.load()\n",
    "trainer.fit()\n",
    "trainer.save('./parameters/' +  'cross_validation_finetune.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = Generator(in_channels=128, out_channels=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(g_model,\n",
    "                  d_model,\n",
    "                  trainer_kwargs={'max_epochs': 200})\n",
    "trainer.fit()\n",
    "trainer.save('./parameters/' + 'cross_validation_proposed_pretrain.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "law",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
