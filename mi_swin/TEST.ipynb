{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/conda/envs/law/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from time_dataset import  batch_generator, batch_mask\n",
    "from visualization_metrics import visualization\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        output_size,\n",
    "        num_layers,\n",
    "        rnn=nn.GRU,\n",
    "        activation_fn=torch.sigmoid,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rnn = rnn(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.linear(x)\n",
    "        if self.activation_fn is not None:\n",
    "            x = self.activation_fn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(\n",
    "    batch_size,\n",
    "    max_steps,\n",
    "    dataset,\n",
    "    device,\n",
    "    embedder,\n",
    "    generator,\n",
    "    supervisor,\n",
    "    recovery,\n",
    "    discriminator,\n",
    "    gamma,\n",
    "    ratio\n",
    "):\n",
    "    def _loss_e_t0(x_tilde, x):\n",
    "        return F.mse_loss(x_tilde, x)\n",
    "\n",
    "    def _loss_e_0(loss_e_t0):\n",
    "        return torch.sqrt(loss_e_t0) * 10\n",
    "\n",
    "    def _loss_e(loss_e_0, loss_s):\n",
    "        return loss_e_0 + 0.1 * loss_s\n",
    "\n",
    "    def _loss_s(h_hat_supervise, h):\n",
    "        return F.mse_loss(h[:, 1:, :], h_hat_supervise[:, 1:, :])\n",
    "\n",
    "    def _loss_g_u(y_fake):\n",
    "        return F.binary_cross_entropy_with_logits(y_fake, torch.ones_like(y_fake))\n",
    "\n",
    "    def _loss_g_u_e(y_fake_e):\n",
    "        return F.binary_cross_entropy_with_logits(y_fake_e, torch.ones_like(y_fake_e))\n",
    "\n",
    "    def _loss_g_v(x_hat, x):\n",
    "        loss_g_v1 = torch.mean(\n",
    "            torch.abs(torch.sqrt(torch.var(x_hat, 0) + 1e-6) - torch.sqrt(torch.var(x, 0) + 1e-6))\n",
    "        )\n",
    "        loss_g_v2 = torch.mean(torch.abs(torch.mean(x_hat, 0) - torch.mean(x, 0)))\n",
    "        return loss_g_v1 + loss_g_v2\n",
    "\n",
    "    def _loss_g(loss_g_u, loss_g_u_e, loss_s, loss_g_v):\n",
    "        return loss_g_u + gamma * loss_g_u_e + 100 * torch.sqrt(loss_s) + 100 * loss_g_v\n",
    "\n",
    "    def _loss_d(y_real, y_fake, y_fake_e):\n",
    "        loss_d_real = F.binary_cross_entropy_with_logits(y_real, torch.ones_like(y_real))\n",
    "        loss_d_fake = F.binary_cross_entropy_with_logits(y_fake, torch.zeros_like(y_fake))\n",
    "        loss_d_fake_e = F.binary_cross_entropy_with_logits(y_fake_e, torch.zeros_like(y_fake_e))\n",
    "        return loss_d_real + loss_d_fake + gamma * loss_d_fake_e\n",
    "\n",
    "    optimizer_er = optim.Adam(chain(embedder.parameters(), recovery.parameters()))\n",
    "    optimizer_gs = optim.Adam(chain(generator.parameters(), supervisor.parameters()))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters())\n",
    "\n",
    "    embedder.train()\n",
    "    generator.train()\n",
    "    supervisor.train()\n",
    "    recovery.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    print(\"Start Embedding Network Training\")\n",
    "    for step in range(1, max_steps + 1):\n",
    "        x = batch_generator(dataset, batch_size).to(device)\n",
    "        # print(x.shape)\n",
    "        h = embedder(x)\n",
    "        x_tilde = recovery(h)\n",
    "\n",
    "        loss_e_t0 = _loss_e_t0(x_tilde, x)\n",
    "        loss_e_0 = _loss_e_0(loss_e_t0)\n",
    "        optimizer_er.zero_grad()\n",
    "        loss_e_0.backward()\n",
    "        optimizer_er.step()\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(\n",
    "                \"step: \"\n",
    "                + str(step)\n",
    "                + \"/\"\n",
    "                + str(max_steps)\n",
    "                + \", loss_e: \"\n",
    "                + str(np.round(np.sqrt(loss_e_t0.item()), 4))\n",
    "            )\n",
    "    print(\"Finish Embedding Network Training\")\n",
    "\n",
    "    print(\"Start Training with Supervised Loss Only\")\n",
    "    for step in range(1, max_steps + 1):\n",
    "        # x = batch_generator(dataset, batch_size).to(device)\n",
    "        # _,z,_ = batch_mask(x, ratio)\n",
    "        x,_,z,_ = batch_mask(dataset, batch_size, ratio)\n",
    "        x = x.to(device)\n",
    "\n",
    "        h = embedder(x) \n",
    "        h_hat_supervise = supervisor(h)\n",
    "\n",
    "        loss_s = _loss_s(h_hat_supervise, h)\n",
    "        optimizer_gs.zero_grad()\n",
    "        loss_s.backward()\n",
    "        optimizer_gs.step()\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(\n",
    "                \"step: \"\n",
    "                + str(step)\n",
    "                + \"/\"\n",
    "                + str(max_steps)\n",
    "                + \", loss_s: \"\n",
    "                + str(np.round(np.sqrt(loss_s.item()), 4))\n",
    "            )\n",
    "    print(\"Finish Training with Supervised Loss Only\")\n",
    "\n",
    "    print(\"Start Joint Training\")\n",
    "    for step in range(1, max_steps + 1):\n",
    "        for _ in range(2):\n",
    "            # x = batch_generator(dataset, batch_size).to(device)\n",
    "            # z = torch.randn(batch_size, x.size(1), x.size(2)).to(device)\n",
    "            x,_,_,z = batch_mask(dataset, batch_size, ratio)\n",
    "            x = x.to(device)\n",
    "            z = z.to(device)\n",
    "\n",
    "            h = embedder(x)\n",
    "            e_hat = generator(z)\n",
    "            h_hat = supervisor(e_hat)\n",
    "            h_hat_supervise = supervisor(h)\n",
    "            x_hat = recovery(h_hat)\n",
    "            y_fake = discriminator(h_hat)\n",
    "            y_fake_e = discriminator(e_hat)\n",
    "\n",
    "            loss_s = _loss_s(h_hat_supervise, h)\n",
    "            loss_g_u = _loss_g_u(y_fake)\n",
    "            loss_g_u_e = _loss_g_u_e(y_fake_e)\n",
    "            loss_g_v = _loss_g_v(x_hat, x)\n",
    "            loss_g = _loss_g(loss_g_u, loss_g_u_e, loss_s, loss_g_v)\n",
    "            optimizer_gs.zero_grad()\n",
    "            loss_g.backward()\n",
    "            optimizer_gs.step()\n",
    "\n",
    "            h = embedder(x)\n",
    "            x_tilde = recovery(h)\n",
    "            h_hat_supervise = supervisor(h)\n",
    "\n",
    "            loss_e_t0 = _loss_e_t0(x_tilde, x)\n",
    "            loss_e_0 = _loss_e_0(loss_e_t0)\n",
    "            loss_s = _loss_s(h_hat_supervise, h)\n",
    "            loss_e = _loss_e(loss_e_0, loss_s)\n",
    "            optimizer_er.zero_grad()\n",
    "            loss_e.backward()\n",
    "            optimizer_er.step()\n",
    "\n",
    "        # x = batch_generator(dataset, batch_size).to(device)\n",
    "        # z = torch.randn(batch_size, x.size(1), x.size(2)).to(device)\n",
    "        x,_,_,z = batch_mask(dataset, batch_size, ratio)\n",
    "        x = x.to(device)\n",
    "        z = z.to(device)\n",
    "\n",
    "        h = embedder(x)\n",
    "        e_hat = generator(z)\n",
    "        h_hat = supervisor(e_hat)\n",
    "        y_fake = discriminator(h_hat)\n",
    "        y_real = discriminator(h)\n",
    "        y_fake_e = discriminator(e_hat)\n",
    "\n",
    "        loss_d = _loss_d(y_real, y_fake, y_fake_e)\n",
    "        if loss_d.item() > 0.15:\n",
    "            optimizer_d.zero_grad()\n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(\n",
    "                \"step: \"\n",
    "                + str(step)\n",
    "                + \"/\"\n",
    "                + str(max_steps)\n",
    "                + \", loss_d: \"\n",
    "                + str(np.round(loss_d.item(), 4))\n",
    "                + \", loss_g_u: \"\n",
    "                + str(np.round(loss_g_u.item(), 4))\n",
    "                + \", loss_g_v: \"\n",
    "                + str(np.round(loss_g_v.item(), 4))\n",
    "                + \", loss_s: \"\n",
    "                + str(np.round(np.sqrt(loss_s.item()), 4))\n",
    "                + \", loss_e_t0: \"\n",
    "                + str(np.round(np.sqrt(loss_e_t0.item()), 4))\n",
    "            )\n",
    "    print(\"Finish Joint Training\")\n",
    "\n",
    "\n",
    "def visualize(dataset, device, generator, supervisor, recovery, batch_size, ratio):\n",
    "    # generator.load_state_dict(torch.load(\"generator.pt\"))\n",
    "    # supervisor.load_state_dict(torch.load(\"supervisor.pt\"))\n",
    "    # recovery.load_state_dict(torch.load(\"recovery.pt\"))\n",
    "    seq_len = dataset[0].shape[0]\n",
    "    input_size = dataset[0].shape[1]\n",
    "    dataset_size = 10*len(dataset)\n",
    "\n",
    "    generator.eval()\n",
    "    supervisor.eval()\n",
    "    recovery.eval()\n",
    "    \n",
    "    if dataset_size > batch_size:\n",
    "        n = int(dataset_size/batch_size)\n",
    "        # z = torch.Tensor([])\n",
    "        # e_hat = torch.Tensor([])\n",
    "        # h_hat = torch.Tensor([])\n",
    "        x_hat = torch.Tensor([])\n",
    "        x_origin = torch.Tensor([])\n",
    "        for i in range(n):\n",
    "            z0,_,_,z = batch_mask(dataset, batch_size, ratio)\n",
    "            x_origin = torch.cat((x_origin, z0), dim=0)\n",
    "            # z = torch.cat((z, zp), dim=0)\n",
    "            z = z.to(device)\n",
    "            with torch.no_grad():\n",
    "                e_hat = generator(z)\n",
    "                h_hat = supervisor(e_hat)\n",
    "                x_hat = torch.cat((x_hat, (recovery(h_hat).cpu())), dim=0)\n",
    "                \n",
    "        if dataset_size/n -batch_size !=0:\n",
    "            z0,_,_,z = batch_mask(dataset, dataset_size-n*batch_size, ratio)\n",
    "            x_origin = torch.cat((x_origin, z0), dim=0)\n",
    "            z = z.to(device)\n",
    "            with torch.no_grad():\n",
    "                e_hat = generator(z)\n",
    "                h_hat = supervisor(e_hat)\n",
    "                x_hat = torch.cat((x_hat, (recovery(h_hat).cpu())), dim=0)\n",
    "            \n",
    "    else:\n",
    "        z0,_,_,z = batch_mask(dataset, dataset_size, ratio)\n",
    "        x_origin = torch.cat((x_origin, z0), dim=0)\n",
    "        z = z.to(device)\n",
    "        with torch.no_grad():\n",
    "            e_hat = generator(z)\n",
    "            h_hat = supervisor(e_hat)\n",
    "            x_hat = recovery(h_hat)\n",
    "    \n",
    "\n",
    "\n",
    "    # visualization(dataset, generated_data_curr, \"pca\")\n",
    "    # visualization(dataset, generated_data_curr, \"tsne\")\n",
    "    \n",
    "    return generated_data_curr, generated_data_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Net(input_size=30,hidden_size=96,output_size=96,num_layers=2,rnn=nn.GRU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 500, 96])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5,500,30)\n",
    "y = embedder(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Net(30, 96, 96, 2, rnn=nn.GRU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor = Net(96, 96, 96, 2, rnn=nn.GRU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 500, 96])\n"
     ]
    }
   ],
   "source": [
    "y2 = supervisor(y)\n",
    "print(y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery = Net(96, 96, 30, 2, rnn=nn.GRU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 500, 30])\n"
     ]
    }
   ],
   "source": [
    "y3 = recovery(y2)\n",
    "print(y3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Net(96, 96, 1, 3, rnn=nn.GRU, activation_fn=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 500, 1])\n"
     ]
    }
   ],
   "source": [
    "output = discriminator(y2)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-06 14:56:39] INFO (torcheeg/MainThread) ğŸ” | Processing EEG data. Processed EEG data has been cached to \u001b[92m.torcheeg/datasets_1733468199358_CtIn7\u001b[0m.\n",
      "[2024-12-06 14:56:39] INFO (torcheeg/MainThread) â³ | Monitoring the detailed processing of a record for debugging. The processing of other records will only be reported in percentage to keep it clean.\n",
      "[PROCESS]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 64.00it/s]\n",
      "\n",
      "[RECORD ./subdataset/sourcedata/sub-50/sub-50_task-motor-imagery_eeg.mat]: 0it [00:00, ?it/s]\u001b[A\n",
      "[RECORD ./subdataset/sourcedata/sub-50/sub-50_task-motor-imagery_eeg.mat]: 1it [00:00,  4.55it/s]\u001b[A\n",
      "[RECORD ./subdataset/sourcedata/sub-50/sub-50_task-motor-imagery_eeg.mat]: 17it [00:00, 64.60it/s]\u001b[A\n",
      "[RECORD ./subdataset/sourcedata/sub-50/sub-50_task-motor-imagery_eeg.mat]: 33it [00:00, 97.26it/s]\u001b[A\n",
      "[RECORD ./subdataset/sourcedata/sub-50/sub-50_task-motor-imagery_eeg.mat]: 46it [00:00, 90.39it/s]\u001b[A\n",
      "[RECORD ./subdataset/sourcedata/sub-50/sub-50_task-motor-imagery_eeg.mat]: 57it [00:00, 91.47it/s]\u001b[A\n",
      "[RECORD ./subdataset/sourcedata/sub-50/sub-50_task-motor-imagery_eeg.mat]: 70it [00:00, 101.62it/s]\u001b[A\n",
      "[RECORD ./subdataset/sourcedata/sub-50/sub-50_task-motor-imagery_eeg.mat]: 84it [00:00, 111.71it/s]\u001b[A\n",
      "[RECORD ./subdataset/sourcedata/sub-50/sub-50_task-motor-imagery_eeg.mat]: 99it [00:01, 120.81it/s]\u001b[A\n",
      "[RECORD ./subdataset/sourcedata/sub-50/sub-50_task-motor-imagery_eeg.mat]: 112it [00:01, 98.54it/s]\u001b[A\n",
      "[RECORD ./subdataset/sourcedata/sub-50/sub-50_task-motor-imagery_eeg.mat]: 123it [00:01, 85.40it/s]\u001b[A\n",
      "[RECORD ./subdataset/sourcedata/sub-50/sub-50_task-motor-imagery_eeg.mat]: 133it [00:01, 87.53it/s]\u001b[A\n",
      "[RECORD ./subdataset/sourcedata/sub-50/sub-50_task-motor-imagery_eeg.mat]: 149it [00:01, 104.64it/s]\u001b[A\n",
      "[RECORD ./subdataset/sourcedata/sub-50/sub-50_task-motor-imagery_eeg.mat]: 165it [00:01, 118.35it/s]\u001b[A\n",
      "[RECORD ./subdataset/sourcedata/sub-50/sub-50_task-motor-imagery_eeg.mat]: 182it [00:01, 131.29it/s]\u001b[A\n",
      "[RECORD ./subdataset/sourcedata/sub-50/sub-50_task-motor-imagery_eeg.mat]: 198it [00:01, 138.82it/s]\u001b[A\n",
      "                                                                                                    \u001b[A[2024-12-06 14:56:43] INFO (torcheeg/MainThread) âœ… | All processed EEG data has been cached to .torcheeg/datasets_1733468199358_CtIn7.\n",
      "[2024-12-06 14:56:43] INFO (torcheeg/MainThread) ğŸ˜Š | Please set \u001b[92mio_path\u001b[0m to \u001b[92m.torcheeg/datasets_1733468199358_CtIn7\u001b[0m for the next run, to directly read from the cache if you wish to skip the data processing step.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 500, 30)\n",
      "0\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "from strokes import StrokePatientsMIDataset\n",
    "import scipy\n",
    "from torcheeg.transforms import Select,BandSignal,Compose,ToTensor\n",
    "from typing import Callable, Dict, Union, List\n",
    "import numpy as np\n",
    "import soxr\n",
    "from downsample import SetSamplingRate\n",
    "from baseline import BaselineCorrection\n",
    "from base_transform import EEGTransform\n",
    "\n",
    "dataset = StrokePatientsMIDataset(root_path='./subdataset',\n",
    "                                #  io_path='.torcheeg/datasets_1733374842301_f9t8X',\n",
    "                        chunk_size=500,  # 1 second\n",
    "                        overlap = 0,\n",
    "                        offline_transform=Compose(\n",
    "                                [BaselineCorrection(),\n",
    "                                BandSignal(sampling_rate=484,band_dict={'frequency_range':[8,40]})]),\n",
    "                        # online_transform=Compose(\n",
    "                        #         [ToTensor()]),\n",
    "                \n",
    "                        label_transform=Select('label'),\n",
    "                        num_worker=8\n",
    ")\n",
    "print(dataset[0][0].shape) #EEG shape(1,30,500)\n",
    "print(dataset[0][1])  # label (int)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Embedding Network Training\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot index by location index with a non-integer key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m Net(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m96\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m96\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, rnn\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mGRU)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# å¼€å§‹è®­ç»ƒ\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m      \u001b[49m\u001b[43membedder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msupervisor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msupervisor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecovery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecovery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# å¯è§†åŒ–ç»“æœ\u001b[39;00m\n\u001b[1;32m     22\u001b[0m generated_data_curr, generated_data_origin \u001b[38;5;241m=\u001b[39m visualize(dataset, device, generator, supervisor, recovery, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 94\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(batch_size, max_steps, dataset, device, embedder, generator, supervisor, recovery, discriminator, gamma, ratio)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart Embedding Network Training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_steps \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 94\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     h \u001b[38;5;241m=\u001b[39m embedder(x)\n",
      "File \u001b[0;32m~/autodl-tmp/.autodl/kinlaw/mi_swin/time_dataset.py:20\u001b[0m, in \u001b[0;36mbatch_generator\u001b[0;34m(dataset, batch_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandperm(dataset_size)\n\u001b[1;32m     19\u001b[0m batch_idx \u001b[38;5;241m=\u001b[39m idx[:batch_size]\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m---> 20\u001b[0m batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([to_tensor(dataset[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_idx])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m~/autodl-tmp/.autodl/kinlaw/mi_swin/time_dataset.py:20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandperm(dataset_size)\n\u001b[1;32m     19\u001b[0m batch_idx \u001b[38;5;241m=\u001b[39m idx[:batch_size]\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m---> 20\u001b[0m batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([to_tensor(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_idx])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m~/autodl-tmp/.autodl/kinlaw/mi_swin/strokes.py:257\u001b[0m, in \u001b[0;36mStrokePatientsMIDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple:\n\u001b[0;32m--> 257\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     eeg_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    259\u001b[0m     eeg_record \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_record_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/autodl-tmp/.autodl/kinlaw/mi_swin/base_dataset.py:435\u001b[0m, in \u001b[0;36mBaseDataset.read_info\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_info\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m    424\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Query the corresponding meta information in the MetaInfoIO according to the the given :obj:`index`.\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m        dict: The meta information.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[0;32m~/autodl-tmp/conda/envs/law/lib/python3.8/site-packages/pandas/core/indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1100\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1102\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/conda/envs/law/lib/python3.8/site-packages/pandas/core/indexing.py:1653\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1651\u001b[0m key \u001b[38;5;241m=\u001b[39m item_from_zerodim(key)\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_integer(key):\n\u001b[0;32m-> 1653\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot index by location index with a non-integer key"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from time_dataset import batch_generator, batch_mask  # å‡è®¾ä½ å·²å®ç°çš„æ‰¹æ¬¡ç”Ÿæˆå™¨\n",
    "from visualization_metrics import visualization  # å‡è®¾å·²å®ç°å¯è§†åŒ–å‡½æ•°\n",
    "\n",
    "# å‡è®¾æ•°æ®é›†å’Œè®¾å¤‡å·²å‡†å¤‡å¥½\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# dataset = ...  # åŠ è½½æ•°æ®é›†\n",
    "\n",
    "# åˆå§‹åŒ–ç½‘ç»œ\n",
    "embedder = Net(input_size=30, hidden_size=96, output_size=96, num_layers=2, rnn=nn.GRU)\n",
    "generator = Net(input_size=30, hidden_size=96, output_size=96, num_layers=2, rnn=nn.GRU)\n",
    "supervisor = Net(input_size=96, hidden_size=96, output_size=96, num_layers=2, rnn=nn.GRU)\n",
    "recovery = Net(input_size=96, hidden_size=96, output_size=30, num_layers=2, rnn=nn.GRU)\n",
    "discriminator = Net(input_size=96, hidden_size=96, output_size=1, num_layers=2, rnn=nn.GRU)\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "train(batch_size=64, max_steps=10000, dataset=dataset, device=device,\n",
    "      embedder=embedder, generator=generator, supervisor=supervisor, recovery=recovery, \n",
    "      discriminator=discriminator, gamma=0.1, ratio=0.3)\n",
    "\n",
    "# å¯è§†åŒ–ç»“æœ\n",
    "generated_data_curr, generated_data_origin = visualize(dataset, device, generator, supervisor, recovery, batch_size=64, ratio=0.3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "law",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
